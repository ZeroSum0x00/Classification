Model: "DINOv2-Small-14"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_45 (InputLayer)       [(None, 224, 224, 3)]        0         []                            
                                                                                                  
 stem_conv (PatchConv2DWith  (None, 16, 16, 384)          226176    ['input_45[0][0]']            
 ResampleWeights)                                                                                 
                                                                                                  
 reshape_161 (Reshape)       (None, 256, 384)             0         ['stem_conv[0][0]']           
                                                                                                  
 cls_token (ClassToken)      (None, 257, 384)             384       ['reshape_161[0][0]']         
                                                                                                  
 positional_embedding (Posi  (None, 257, 384)             98688     ['cls_token[0][0]']           
 tionalEmbedding)                                                                                 
                                                                                                  
 AttentionMLP_block_1 (Atte  (None, 257, 384)             1775232   ['positional_embedding[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_2 (Atte  (None, 257, 384)             1775232   ['AttentionMLP_block_1[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_3 (Atte  (None, 257, 384)             1775232   ['AttentionMLP_block_2[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_4 (Atte  (None, 257, 384)             1775232   ['AttentionMLP_block_3[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_5 (Atte  (None, 257, 384)             1775232   ['AttentionMLP_block_4[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_6 (Atte  (None, 257, 384)             1775232   ['AttentionMLP_block_5[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_7 (Atte  (None, 257, 384)             1775232   ['AttentionMLP_block_6[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_8 (Atte  (None, 257, 384)             1775232   ['AttentionMLP_block_7[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_9 (Atte  (None, 257, 384)             1775232   ['AttentionMLP_block_8[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_10 (Att  (None, 257, 384)             1775232   ['AttentionMLP_block_9[0][0]']
 entionMLPBlock)                                                                                  
                                                                                                  
 AttentionMLP_block_11 (Att  (None, 257, 384)             1775232   ['AttentionMLP_block_10[0][0]'
 entionMLPBlock)                                                    ]                             
                                                                                                  
 AttentionMLP_block_12 (Att  (None, 257, 384)             1775232   ['AttentionMLP_block_11[0][0]'
 entionMLPBlock)                                                    ]                             
                                                                                                  
 out_ln (LayerNormalization  (None, 257, 384)             768       ['AttentionMLP_block_12[0][0]'
 )                                                                  ]                             
                                                                                                  
 tf.__operators__.getitem_1  (None, 256, 384)             0         ['out_ln[0][0]']              
 60 (SlicingOpLambda)                                                                             
                                                                                                  
 tf.__operators__.getitem_1  (None, 384)                  0         ['out_ln[0][0]']              
 59 (SlicingOpLambda)                                                                             
                                                                                                  
 tf.math.reduce_mean_11 (TF  (None, 384)                  0         ['tf.__operators__.getitem_160
 OpLambda)                                                          [0][0]']                      
                                                                                                  
 tf.concat_4 (TFOpLambda)    (None, 768)                  0         ['tf.__operators__.getitem_159
                                                                    [0][0]',                      
                                                                     'tf.math.reduce_mean_11[0][0]
                                                                    ']                            
                                                                                                  
 predictions (Dense)         (None, 1000)                 769000    ['tf.concat_4[0][0]']         
                                                                                                  
 activation_1329 (Activatio  (None, 1000)                 0         ['predictions[0][0]']         
 n)                                                                                               
                                                                                                  
==================================================================================================
Total params: 22397800 (85.44 MB)
Trainable params: 22397800 (85.44 MB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
