Model: "DINOv2-Base-14"
__________________________________________________________________________________________________
 Layer (type)                Output Shape                 Param #   Connected to                  
==================================================================================================
 input_41 (InputLayer)       [(None, 224, 224, 3)]        0         []                            
                                                                                                  
 stem_conv (PatchConv2DWith  (None, 16, 16, 768)          452352    ['input_41[0][0]']            
 ResampleWeights)                                                                                 
                                                                                                  
 reshape_157 (Reshape)       (None, 256, 768)             0         ['stem_conv[0][0]']           
                                                                                                  
 cls_token (ClassToken)      (None, 257, 768)             768       ['reshape_157[0][0]']         
                                                                                                  
 positional_embedding (Posi  (None, 257, 768)             197376    ['cls_token[0][0]']           
 tionalEmbedding)                                                                                 
                                                                                                  
 AttentionMLP_block_1 (Atte  (None, 257, 768)             7089408   ['positional_embedding[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_2 (Atte  (None, 257, 768)             7089408   ['AttentionMLP_block_1[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_3 (Atte  (None, 257, 768)             7089408   ['AttentionMLP_block_2[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_4 (Atte  (None, 257, 768)             7089408   ['AttentionMLP_block_3[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_5 (Atte  (None, 257, 768)             7089408   ['AttentionMLP_block_4[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_6 (Atte  (None, 257, 768)             7089408   ['AttentionMLP_block_5[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_7 (Atte  (None, 257, 768)             7089408   ['AttentionMLP_block_6[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_8 (Atte  (None, 257, 768)             7089408   ['AttentionMLP_block_7[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_9 (Atte  (None, 257, 768)             7089408   ['AttentionMLP_block_8[0][0]']
 ntionMLPBlock)                                                                                   
                                                                                                  
 AttentionMLP_block_10 (Att  (None, 257, 768)             7089408   ['AttentionMLP_block_9[0][0]']
 entionMLPBlock)                                                                                  
                                                                                                  
 AttentionMLP_block_11 (Att  (None, 257, 768)             7089408   ['AttentionMLP_block_10[0][0]'
 entionMLPBlock)                                                    ]                             
                                                                                                  
 AttentionMLP_block_12 (Att  (None, 257, 768)             7089408   ['AttentionMLP_block_11[0][0]'
 entionMLPBlock)                                                    ]                             
                                                                                                  
 out_ln (LayerNormalization  (None, 257, 768)             1536      ['AttentionMLP_block_12[0][0]'
 )                                                                  ]                             
                                                                                                  
 tf.__operators__.getitem_1  (None, 256, 768)             0         ['out_ln[0][0]']              
 52 (SlicingOpLambda)                                                                             
                                                                                                  
 tf.__operators__.getitem_1  (None, 768)                  0         ['out_ln[0][0]']              
 51 (SlicingOpLambda)                                                                             
                                                                                                  
 tf.math.reduce_mean_7 (TFO  (None, 768)                  0         ['tf.__operators__.getitem_152
 pLambda)                                                           [0][0]']                      
                                                                                                  
 tf.concat (TFOpLambda)      (None, 1536)                 0         ['tf.__operators__.getitem_151
                                                                    [0][0]',                      
                                                                     'tf.math.reduce_mean_7[0][0]'
                                                                    ]                             
                                                                                                  
 predictions (Dense)         (None, 1000)                 1537000   ['tf.concat[0][0]']           
                                                                                                  
 activation_1325 (Activatio  (None, 1000)                 0         ['predictions[0][0]']         
 n)                                                                                               
                                                                                                  
==================================================================================================
Total params: 87261928 (332.88 MB)
Trainable params: 87261928 (332.88 MB)
Non-trainable params: 0 (0.00 Byte)
__________________________________________________________________________________________________
